points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "Ulala_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
install.packages("MASS")
library(MASS)
#"Куканьков" - 9 букв
#Моделируем данные
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
#Кластерный анализ cl
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "Ulala_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "Ulala_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
surname<-dubovskij
dubovskij<-"dubovskij"
dubovskij
dubovskij.length
dubovskij.size
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
gc()
View(mod2)
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
#Кластерный анализ cl
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "Ulala_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "Ulala_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
library(MASS)
#"Дубовский" - 9 букв
#Моделируем данные
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
#Кластерный анализ cl
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
library(MASS)
x1 <- rnorm(10, mean = 0, sd = 5/3)
x2 <- rnorm(20, 5, 5/3)
y1 <- rnorm(10, 0, 5/3)
y2 <- rnorm(20, 5, 5/3)
xy <- cbind(c(x1, x2),c(y1, y2))
xy
n <- 30
n.train<-floor(n*0.7)
n.test<-n-n.train
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl<-kmeans(xy,2)
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
mod<-qda(data.train, cl.train)
cl.test_est<-predict(mod, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idx<-idx.test[cl.test_est!=cl.test]
plot(xy, type="n")
points(data.train,pch=24, col=ifelse(cl.train==1,"blue","orange"))
points(data.test,pch=21, col=ifelse(cl.test==1,"blue","orange"))
legend("topleft",legend=c("train - 1","train - 2"),pch=24,col=c("blue","orange"))
legend("bottomright",legend=c("test - 1","test - 2"),pch=21,col=c("blue","orange"))
if (length(idx)==1){
points(xy[idx,1],xy[idx,2],col="red", pch=4)
}else
points(xy[idx,],col="red", pch=4)
legend("bottom",legend=c("wrong"),pch=4,col="red")
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd)
cl.train[i]=ifelse(cl.train[i]==1,2,1)
mod2<-qda(data.train, cl.train)
cl.test_est<-predict(mod2, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idx2<-idx.test[cl.test_est!=cl.test]
plot(xy, type="n")
points(data.train,pch=24, col=ifelse(cl.train==1,"blue","orange"))
points(data.test,pch=21, col=ifelse(cl.test==1,"blue","orange"))
legend("topleft",legend=c("train - 1","train - 2"),pch=24,col=c("blue","orange"))
legend("bottomright",legend=c("test - 1","test - 2"),pch=21,col=c("blue","orange"))
if (length(idx2)== 1){
points(xy[idx2,1],xy[idx2,2],col="red", pch=4)
}else
points(xy[idx2,],col="red", pch=4)
legend("bottom",legend=c("mistake"),pch=4,col="red")
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
cat(params$surname, params$name)
cat(params$variant)
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
dat
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
dat
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
cat(params$surname, params$name)
cat(params$variant)
cat(params$variant)
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
dat
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "2 clusters")
legend("bottomleft", legend = c("1", "2"), fill = c("blue", "green"))
library("MASS")
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
dat
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "2 clusters")
legend("bottomleft", legend = c("1", "2"), fill = c("blue", "green"))
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
dat
dat_1 = cbind(rnorm(20, 0 , 9 / 3), rnorm(20, 0 ,  9 /3))
dat_2 = cbind(rnorm(10, 9 , 9 / 3), rnorm(10, 9 ,  9 /3))
dat = rbind(dat_1, dat_2)
dat
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "2 clusters")
legend("bottomleft", legend = c("1", "2"), fill = c("blue", "green"))
library("MASS")
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
cl = kmeans(scale(dat), 2)
sapply(1:2, function(i) cl$centers[,i] * sd(dat[,i]) + mean(dat[,i]))
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "2 clusters")
legend("bottomleft", legend = c("1", "2"), fill = c("blue", "green"))
library("MASS")
#Дискриминантный анализ № 1
n = length(dat[,1])
n.train = floor(n * 0.7)
n.test = n - n.train
idx.train = sample(1:n, n.train)
data.train = dat[idx.train, ]
idx.test = setdiff(1:n, idx.train)
data.test = dat[idx.test, ]
cl.train = factor(cl[["cluster"]][idx.train])
cl.test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod = qda(data.train, cl.train)
cl.test_est = predict(mod, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est != cl.test) / n.test
idx.wrong = idx.test[cl.test_est != cl.test]
#Строим первый график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_1")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong, 1], dat[idx.wrong, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.train,],pch = 2, cex = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
#Меняем номер кластера в 20% выборки
idx.new = sample(1:n, n*0.2)
for (i in 1:length(idx.new)) {
if(cl[["cluster"]][idx.new[i]] == 1){
cl[["cluster"]][idx.new[i]] = 2}
else{
cl[["cluster"]][idx.new[i]] = 1
}
}
#Дискриминантный анализ № 2
cl_train = factor(cl[["cluster"]][idx.train])
cl_test = factor(cl[["cluster"]][idx.test])
#Обучение полученой тестовой выборки
mod2 = qda(data.train, cl.train)
cl.test_est2 = predict(mod2, data.test)$class
#Оценка ошибки классификации
sum(cl.test_est2 != cl.test) / n.test
idx.wrong2 = idx.test[cl.test_est2 != cl.test]
#Строим второй график
plot(dat, col = c("blue", "green")[cl$cluster], pch = 20, cex = 2, xlab = "X", ylab = "Y", main = "plot_2")
legend("topleft", legend = c("1", "2"), fill = c("blue", "green"))
points(dat[idx.wrong2, 1], dat[idx.wrong2, 2], col = "red", pch = 4, cex = 2, lwd = 3)
points(dat[idx.new, ], pch = 3, cex = 2, lwd = 2)
